# -*- coding: utf-8 -*-
"""DecesionTree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-L4B8KuzWMArBKMn_84IYASAJWwnhBFV
"""

#all libraries required for model implementation Decesion tree
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score # test train split
from sklearn.preprocessing import StandardScaler # feature standaridazation
from sklearn.decomposition import PCA # principal component analysis
from sklearn.tree import DecisionTreeClassifier # for mpodel training decesion tree
from sklearn.feature_selection import RFE # for feature selection
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report # fro model evaluation
from imblearn.over_sampling import SMOTE # to deal with class imbalance
import seaborn as sns # for plotting
import matplotlib.pyplot as plt # for plotiing

# the risk function to assign the risk category to each class
def heartrisk_function(p):
    if ( p["cardio"] == 0 and p["cholesterol"] <= 1 and p["gluc"] <= 1 and p["ap_hi"] <= 120 and p["ap_lo"] <= 80 and p["alco"] <= 0 and p["smoke"] <= 0):
        return 0
    elif ( p["cardio"] == 0 and p["cholesterol"] <= 2 and p["gluc"] <= 2 and p["ap_hi"] <= 130 and p["ap_lo"] <= 85):
        return 1
    elif ( p["cardio"] <= 1 and p["cholesterol"] <= 3 and p["gluc"] <= 3  and 130 < p["ap_hi"] <= 149 and 85 < p["ap_lo"] <= 89):
        return 2
    elif ( p["cardio"] <= 1 and p["cholesterol"] <= 3 and p["gluc"] <= 3 and p["ap_hi"] <= 200 and p["ap_lo"] <= 120 ):
        return 3
    return 4  # invalid instance, nit falling in anyof the category

#the cardio file loading into dataframe
pathof_file = '/content/drive/MyDrive/cardio_train.csv'
df = pd.read_csv(pathof_file, sep=';') # Specify the separator as semicolon

# heartrisk_function to filter through the rows of the dataset
df["heart_risk_level"] = df.apply(heartrisk_function, axis=1)

# 1. Data Preprocessing
df["heart_risk_level"] = df.apply(heartrisk_function, axis=1)

# Remove any entries that are invalid (risk_level == 4)
df = df[df["heart_risk_level"] != 4]

# distribution of the 'heart_risk_level'  column after removing invalid entries
print(df['heart_risk_level'].value_counts())

# Check missing values in the dataset
missing_values = df.isnull().sum()
print("Missing Values in Each Column:")
print(missing_values)


# Separation of features and target label.target label is heart risk level
X = df.iloc[:, :-1]  ## Features , used for the training and prediction of heart risk
Y = df.iloc[:, -1]   ##target label Iam predicting

# # data splitting into training and teting. i split data with 80 to 20 ratio with 80 percent for training and 20 percent for testing , spo that model have enough instances for learning
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=0)

# # SMOTE application for imbalanced dataset, As after appyling the risk criteria the data was highly imbalced towards moderate and high risk, very low instances as compared to other.
smote = SMOTE(random_state=0)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Normalization of data fo better results
standdata = StandardScaler()
X_train_resampled = standdata.fit_transform(X_train_resampled)
X_test = standdata.transform(X_test)

# 5. Feature Selection using Recursive Feature Elimination for better model perfromance
clf = DecisionTreeClassifier(random_state=0)
rfe = RFE(estimator=clf, n_features_to_select=10)
X_train_selected = rfe.fit_transform(X_train_resampled, y_train_resampled)
X_test_selected = rfe.transform(X_test)

# MOdel Training

# 10-Fold Cross-validation for good generalizability
cv_scores = cross_val_score(clf, X_train_selected, y_train_resampled, cv=10, scoring='accuracy')

# Print cross-validation scores and the mean cross-validation accuracy
print(f"Cross-validation accuracy scores: {cv_scores}")
print(f"Mean Cross-validation accuracy: {cv_scores.mean() * 100:.2f}%")

# Training of decesion tree clasifiet
clf.fit(X_train_selected, y_train_resampled)

# Model Performance Evaluation
# Create predictions on the test set
y_pred = clf.predict(X_test_selected)

# Confusion Matrix for better reslts visual
CM = confusion_matrix(y_test, y_pred)


# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)

# Generate classification report (Precision, Recall, F1-Score)
class_report = classification_report(y_test, y_pred, target_names=["No Risk", "Low Risk", "Moderate Risk", "High Risk"], zero_division=0)

# Print metrics
#print(f"Model Accuracy: {accuracy * 100:.2f}%")
print("\nClassification Report:\n", class_report)

# 9. Overall Performance Metrics (Precision, Recall, F1-Score)
# Weighted average metrics (to deal with class imbalance)
report = classification_report(y_test, y_pred, output_dict=True)

# precision, recall, and f1-score (weighted average)
precision = report['weighted avg']['precision']
recall = report['weighted avg']['recall']
f1_score = report['weighted avg']['f1-score']

# Print overall metrics
print(f"\nOverall Model Precision: {precision * 100:.2f}%")
print(f"Overall Model Recall: {recall * 100:.2f}%")
print(f"Overall Model F1-Score: {f1_score * 100:.2f}%")

# heatmap confusion matrix
plt.figure(figsize=(8, 6))  # Set the size of the figure


sns.heatmap(CM, annot=True, fmt='d', cmap='Blues', xticklabels=["No Risk", "Low Risk", "Moderate Risk", "High Risk"],
            yticklabels=["No Risk", "Low Risk", "Moderate Risk", "High Risk"], cbar=True, linewidths=0.5, linecolor='black')

# Set the title and labels
plt.title("Confusion Matrix (Decision Tree)", fontsize=16)
plt.xlabel("Predicted Risk Level", fontsize=12)
plt.ylabel("Actual Risk Level", fontsize=12)

# Show the plot
plt.show()